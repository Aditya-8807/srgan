In Week 1, I studied the original SRGAN paper, which introduced the concept of using Generative Adversarial Networks (GANs) for super-resolution of images. Traditional models relied on pixel-wise MSE loss which gave high PSNR but produced blurry images. SRGAN changed this by introducing a perceptual loss, combining MSE with adversarial loss and VGG-based feature loss. This approach helped generate images that looked much closer to real high-resolution images, especially for 4× upscaling.

In Week 2, I explored ESRGAN (Enhanced SRGAN). It builds on SRGAN and brings multiple improvements. Firstly, it replaces regular residual blocks with Residual-in-Residual Dense Blocks (RRDB), allowing for deeper and more stable training. It also removes batch normalization, which reduces artifacts. Secondly, ESRGAN introduces a Relativistic Discriminator, which doesn’t just say whether an image is real or fake, but whether it is more realistic than another image. Lastly, the perceptual loss is improved by using pre-activation features of the VGG network, giving better brightness and texture consistency.

In Week 3, I learnt about data loading in PyTorch, which is extremely useful when working with large image datasets. Using PyTorch’s Dataset and DataLoader classes, I can now prepare low-resolution and high-resolution image pairs, apply transforms (like cropping, rescaling, etc.), and load them efficiently in batches. This forms the backbone of any deep learning training pipeline, ensuring the model gets clean and structured data during training.

In Week 4, I explored how to visualise training performance using annotated heatmaps in Matplotlib. These heatmaps can be used to show values like loss, PSNR, or SSIM across different epochs or configurations. Annotating each cell with its actual value helps in better understanding the model’s behaviour during training and comparison across experiments.
